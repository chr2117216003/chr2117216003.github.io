<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[AC自动机]]></title>
    <url>%2F2019%2F08%2F15%2F2019-08-15-python-ac-automatic%2F</url>
    <content type="text"><![CDATA[AC自动机的python实现。AC自动机是结合Trie树和KMP的多模式匹配的实现。相对于KMP只能处理单个单词的查找，AC自动机通过构建Trie树和其fail指针的方式来对当前匹配失败的字符进行下一步的匹配，减少了时间复杂度，加快程序运行。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667 class node: def __init__(self): self.fail=None self.isword=False self.next=&#123;&#125; self.word=Noneclass ac_automatic: def __init__(self): self.root = node() def add(self, word): p = self.root for w in word: if w in p.next: p = p.next[w] else: p.next[w] = node() p = p.next[w] p.isword=True p.word=word def judge(self): p = self.root temp = [p] while len(temp) != 0: d = temp.pop(0) for v in d.next.keys(): p = d if p == self.root: v.fail = self.root else: while p.fail != None and v not in p.fail.next: p = p.fail if v in p.fail.next: v.fail = p.fail.next[v] else: v.fail = self.root temp.append(v) def search(self, content): p = self.root final = [] for w in content: while p.fail != None and w not in p.fail.next: p = p.fail if w in p.next: p = p.next[w] else: p = self.root if p.isword == True: final.append(p.word) return finalmodel = ac_automatic()model.add(&quot;her&quot;)model.add(&quot;she&quot;)print(model.search(&quot;dfadooudashefhe2fdl&quot;))]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[downloading images from particular website]]></title>
    <url>%2F2019%2F05%2F27%2F2019-5-27-downloading_images_from_particular_website%2F</url>
    <content type="text"><![CDATA[一些特殊的网站，比如漫画网站，并没有对图片做访问登陆的限制，并且图片访问呈现一定的规律性，就可以通过编程简单爬虫的手段下载。在这边我上传了一个我下载哈哈漫画的示例程序，有兴趣的童鞋可以看看。 1234567891011121314151617181920212223242526272829303132333435363738# 爬取特殊网站图片方法一import requestsimport osimport globimport shutilprefix = &quot;有意思的网址/files/&quot;base_url = &quot;有意思的网址/files/80606/&quot;begin_page = 27956end_page = 63655for page_id in range(begin_page, end_page + 1): url = base_url + str(page_id) + &quot;/&quot; index = 1 response_id = True dir_path = url.replace(prefix, &quot;&quot;) if not os.path.exists(dir_path): os.makedirs(dir_path) while response_id != False: url_path = url + str(index) + &quot;.jpg&quot; response = requests.get(url_path) response_id = response.ok filename = url_path.replace(prefix, &quot;&quot;) if response_id == True: print(url_path, filename) with open(filename,&#x27;wb&#x27;) as f: for chunk in response.iter_content(128): f.write(chunk) index += 1 此外还有一种更加优秀的解决方法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384# 爬取特殊网站图片方法二&quot;&quot;&quot;really used in fetching url from 不可描述的网站&quot;&quot;&quot;import requestsimport globimport shutilfrom selenium import webdriverimport timeimport osimport sysimport refrom bs4 import BeautifulSoupfrom selenium.webdriver.chrome.options import Optionsdef downloading_images(prefix, url): filename = url.replace(prefix, &#x27;&#x27;) basename = os.path.basename(filename) dirname = filename.replace(basename, &#x27;&#x27;) dirurl = url.replace(basename, &#x27;&#x27;) if not os.path.exists(dirname): os.makedirs(dirname) response_id = True index = 1 while response_id != False: url_path = dirurl + str(index) + &quot;.jpg&quot; response = requests.get(url_path) response_id = response.ok filename = dirname + str(index) + &quot;.jpg&quot; if response_id == True: print(url_path, filename) with open(filename,&#x27;wb&#x27;) as f: for chunk in response.iter_content(128): f.write(chunk) index += 1if __name__ == &quot;__main__&quot;: prefix = &quot;不可描述的域名&quot; down_loading_urls = [ [&#x27;不可描述的网站&#x27;, 49] ] for down_loading_url, count in down_loading_urls: chrome_options = Options() chrome_options.add_argument(&#x27;-headless&#x27;) chrome_options.add_argument(&#x27;--disable-gpu&#x27;) browser = webdriver.Chrome(chrome_options = chrome_options) try: print(down_loading_url) browser.get(down_loading_url) time.sleep(4) for num in range(1, count + 1): browser.find_element_by_xpath(&#x27;/html/body/div[2]/div[3]/div[2]/ul/li[%d]/a&#x27;%num).click() time.sleep(4) values = browser.find_elements_by_xpath(&#x27;/html/body/div[2]/article/div/div/img[1]&#x27;)[0].get_attribute(&#x27;data-original&#x27;) downloading_images(prefix, values) browser.back() time.sleep(4) except Exception as e: print(&quot;global&quot;,e) finally: browser.close() 完成之后最好把图片都放一个文件夹，并且放在手机上观看1234567891011121314151617181920# 将图片集中到一个文件夹import osimport shutildirname = &quot;files/80606&quot;dirs = os.listdir(dirname)dirs = [int(value) for value in dirs]dirs.sort()new_dirname = dirname + &quot;all&quot;index = 1if not os.path.exists(new_dirname): os.makedirs(new_dirname)for dir_ in dirs: dir_path = os.path.join(dirname, str(dir_)) for file in os.listdir(dir_path): filename = os.path.join(dir_path, file) shutil.copy(filename, os.path.join(new_dirname, str(index) + &quot;.jpg&quot;)) index += 1 123456789101112131415161718192021222324252627282930313233343536373839import osimport shutildirname = &quot;files/80648&quot;new_dirname = dirname + &quot;all&quot;index = 1if not os.path.exists(new_dirname): os.makedirs(new_dirname)dirs = sorted([int(value) for value in os.listdir(dirname)])t_dir = dirs[0]t_path = os.path.join(dirname, str(t_dir))mark = Falsefor f in os.listdir(t_path): t_t_dir = os.path.join(t_path, f) if os.path.isdir(t_t_dir): mark = True shutil.move(t_t_dir, os.path.join(dirname, f))if mark == True: os.rmdir(t_path)dirs = sorted([int(value) for value in os.listdir(dirname)])for dir_ in dirs: path_dir = os.path.join(dirname, str(dir_))# print(path_dir) dir_ = sorted([int(value.split(&quot;.&quot;)[0]) for value in os.listdir(path_dir)]) for file in dir_: filename = os.path.join(path_dir,str(file) + &quot;.jpg&quot;) new_filename = os.path.join(new_dirname,str(index) + &quot;.jpg&quot;) print(filename, new_filename) shutil.copy(filename,new_filename) index += 1]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SFTGAN test]]></title>
    <url>%2F2019%2F04%2F18%2F2019-4-18-SFTGAN_test%2F</url>
    <content type="text"><![CDATA[SFTGAN是截至到目前在本研究部门内部研究发现的最好的和最优的超分辨放大算法，当然算法都是针对某一个领域，某一个方向的。SFTGAN发现是在艺术画的放大生成过程中是最好的。而ESRGAN在现实生活图片，真实照片上的放大效果比较突出，尤其是颜色比较集中的情况下。以下内容是SFTGAN的测试用例和为了方便使用改写的测试代码。 SFTGAN的github地址：SFTGAN 程序的组织架构如下： 首先说明执行命令运行test_subdir.py(即可运行文件，代码如下)将images下的所有图片放大到4096并放置于images_4096下，将大于1024并小于2048的图片先resize到1024再通过SFTGAN来super resolution，而大于2048的图片直接resize到4096。其中的1表示首先拷贝和处理images的图片为三通道然后再放置于images_4096, 而5代表迭代五次，因为本程序只能通过SFTGAN放大四倍，若要从256的图片放大到4096要两次，更小的要更多次数，保险起见设置为5. 1python test_subdir.py images/ images_4096/ 1 5 2048 4096 1024 以下代码放置于pytorch_test文件夹下，用于将目录下的文件放大到指定大小 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168&#x27;&#x27;&#x27;Segmentation codes for generating segmentation probability maps for SFTGAN&#x27;&#x27;&#x27;import osimport globimport numpy as npimport cv2import sysimport torchimport torchvision.utilsimport timeimport architectures as archimport utilfrom PIL import Image# 通道转换def change_image_channels(input_image_path, output_image_path): image = Image.open(input_image_path) if image.mode == &#x27;RGBA&#x27;: r, g, b, a = image.split() image = Image.merge(&quot;RGB&quot;, (r, g, b)) try: os.remove(output_image_path) except: pass image.save(output_image_path) elif image.mode != &#x27;RGB&#x27;: image = image.convert(&quot;RGB&quot;) try: os.remove(output_image_path) except: pass image.save(output_image_path) else: try: os.remove(output_image_path) except: pass image.save(output_image_path) return image# optionsos.environ[&#x27;CUDA_VISIBLE_DEVICES&#x27;] = &#x27;0&#x27;times = 3channel_mark = 1imgSize = 4096finalSize = 4096minImgSize = 1024imagespath = sys.argv[1] #must end with &quot;/&quot;outputdir = sys.argv[2] #must end with &quot;/&quot;channel_mark = int(sys.argv[3]) #default 1, means change all images to 3 channeltimes = int(sys.argv[4]) #default 3imgSize = int(sys.argv[5])finalSize = int(sys.argv[6])minImgSize = int(sys.argv[7])if not os.path.exists(outputdir): os.makedirs(outputdir)device = torch.device(&#x27;cuda&#x27;) # if you want to run on CPU, change &#x27;cuda&#x27; -&gt; &#x27;cpu&#x27;# device = torch.device(&#x27;cpu&#x27;)model_path = &#x27;/home/t-huch/SFTGAN/pretrained_models/SFTGAN_torch.pth&#x27; # torch versionif &#x27;torch&#x27; in model_path: # torch version model = arch.SFT_Net_torch()else: # pytorch version model = arch.SFT_Net()model.load_state_dict(torch.load(model_path), strict=True)model.eval()model = model.to(device)# load modelseg_model = arch.OutdoorSceneSeg()model_path = &#x27;/home/t-huch/SFTGAN/pretrained_models/segmentation_OST_bic.pth&#x27;seg_model.load_state_dict(torch.load(model_path), strict=True)seg_model.eval()seg_model = seg_model.to(device)print(&#x27;Testing SFTGAN ...&#x27;)print(channel_mark)if channel_mark == 1: for root, dirs, files in os.walk(imagespath): for file in files: start_time = time.time() path = os.path.join(root,file) imgname = os.path.basename(path) subDir = os.path.join(outputdir,root.replace(imagespath, &quot;&quot;)) if not os.path.exists(subDir): os.makedirs(subDir) print(path) change_image_channels(path, os.path.join(subDir,imgname))while times &gt; 0: times -= 1 for root, dirs, files in os.walk(outputdir): for file in files: start_time = time.time() path = os.path.join(root,file) imgname = os.path.basename(path) # read image img = cv2.imread(path, cv2.IMREAD_UNCHANGED) print(img.shape, path) if img.shape[0] &lt;imgSize or img.shape[1] &lt;imgSize: if img.shape[0] &gt; minImgSize or img.shape[1] &gt; minImgSize: img = cv2.resize(img, (minImgSize, minImgSize), interpolation=cv2.INTER_CUBIC) test_img = util.modcrop(img, 8) img = util.modcrop(img, 8) if img.ndim == 2: img = np.expand_dims(img, axis=2) img = torch.from_numpy(np.transpose(img, (2, 0, 1))).float() img_LR = util.imresize(img / 255, 1, antialiasing=True) img = util.imresize(img_LR, 4, antialiasing=True) * 255 img[0] -= 103.939 img[1] -= 116.779 img[2] -= 123.68 img = img.unsqueeze(0) img = img.to(device) with torch.no_grad(): output = seg_model(img).detach().float().cpu().squeeze() test_img = test_img * 1.0 / 255 if test_img.ndim == 2: test_img = np.expand_dims(test_img, axis=2) test_img = torch.from_numpy(np.transpose(test_img[:, :, [2, 1, 0]], (2, 0, 1))).float() img_LR = util.imresize(test_img, 1 , antialiasing=True) img_LR = img_LR.unsqueeze(0) img_LR = img_LR.to(device) seg = output seg = seg.unsqueeze(0) seg = seg.to(device) with torch.no_grad(): output = model((img_LR, seg)).data.float().cpu().squeeze() output = util.tensor2img(output) subDir = os.path.join(outputdir,root.replace(outputdir, &quot;&quot;)) if not os.path.exists(subDir): os.makedirs(subDir) util.save_img(output, os.path.join(subDir,imgname)) print(&quot;time consumption : &#123;&#125;&quot;.format(time.time() - start_time)) elif img.shape[0] == finalSize and img.shape[1] == finalSize: pass# subDir = os.path.join(outputdir,root.replace(outputdir, &quot;&quot;))# if not os.path.exists(subDir):# os.makedirs(subDir)# cv2.imwrite(os.path.join(subDir,imgname), img) else: img = cv2.resize(img, (finalSize, finalSize), interpolation=cv2.INTER_CUBIC) subDir = os.path.join(outputdir,root.replace(outputdir, &quot;&quot;)) if not os.path.exists(subDir): os.makedirs(subDir) cv2.imwrite(os.path.join(subDir,imgname), img)]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[voting program]]></title>
    <url>%2F2018%2F12%2F19%2F2018-12-19-voting-program%2F</url>
    <content type="text"><![CDATA[采用多进程多线程的方式进行特征融合和SVM训练的工作。算法中采用得到的是Sklearn的SVC框架，可以设置gridSearch来控制进程的数量，当然如果多个特征文件的多个组合任务时候。可以用本代码的内容。 1%run simple_voting.py -i S4_2RFH.csv,S4_AthMethPre.csv,S4_KNN.csv,S4_PCP.csv -l 0 -c 5 -n 1 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208# encoding:utf-8import getoptfrom sklearn.preprocessing import MinMaxScalerimport os,timefrom multiprocessing import Process, Managerimport pandas as pdimport numpy as npimport itertoolsfrom sklearn.model_selection import KFold from sklearn import svm# from sklearn.cross_validation import train_test_splitimport mathfrom sklearn.model_selection import *import sklearn.ensemblefrom sklearn import metricsfrom sklearn.metrics import roc_curve, aucimport sysfrom sklearn.model_selection import GridSearchCVimport warnings whole_result=[]input_files=&quot;&quot;whole_dimension=[]default_l = 1cross_validation_value = 10CPU_value = 1opts, args = getopt.getopt(sys.argv[1:], &quot;hi:l:c:n:&quot;, )final_out_to_excel=[]row0 = [u&#x27;特征集&#x27;, u&#x27;样本个数&#x27;, u&#x27;分类器&#x27;, u&#x27;Accuracy&#x27;, u&#x27;Precision&#x27;, u&#x27;Recall&#x27;, u&#x27;SN&#x27;, u&#x27;SP&#x27;, u&#x27;Gm&#x27;, u&#x27;F_measure&#x27;, u&#x27;F_score&#x27;, u&#x27;MCC&#x27;, u&#x27;ROC曲线面积&#x27;, u&#x27;tp&#x27;, u&#x27;fn&#x27;, u&#x27;fp&#x27;, u&#x27;tn&#x27;]final_out_to_excel.append(row0) #above was used to generate xlsx format Excel filefor op, value in opts: if op == &quot;-i&quot;: input_files = str(value) input_files = input_files.replace(&quot; &quot;, &quot;&quot;).split(&#x27;,&#x27;) for input_file in input_files: if input_file == &quot;&quot;: print(&quot;Warning: please insure no blank in your input files !&quot;) sys.exit() elif op == &quot;-l&quot;: if int(value) == 1: default_l = 1 else: default_l = -1 elif op == &quot;-c&quot;: cross_validation_value = int(value) elif op == &quot;-n&quot;: CPU_value = int(value)def performance(labelArr, predictArr): #labelArr[i] is actual value,predictArr[i] is predict value TP = 0.; TN = 0.; FP = 0.; FN = 0. for i in range(len(labelArr)): if labelArr[i] == 1 and predictArr[i] == 1: TP += 1. if labelArr[i] == 1 and predictArr[i] == 0: FN += 1. if labelArr[i] == 0 and predictArr[i] == 1: FP += 1. if labelArr[i] == 0 and predictArr[i] == 0: TN += 1. if (TP + FN)==0: SN=0 else: SN = TP/(TP + FN) #Sensitivity = TP/P and P = TP + FN if (FP+TN)==0: SP=0 else: SP = TN/(FP + TN) #Specificity = TN/N and N = TN + FP if (TP+FP)==0: precision=0 else: precision=TP/(TP+FP) if (TP+FN)==0: recall=0 else: recall=TP/(TP+FN) GM=math.sqrt(recall*SP) #MCC = (TP*TN-FP*FN)/math.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)) return precision,recall,SN,SP,GM,TP,TN,FP,FNdef worker(X_train, y_train, cross_validation_value, CPU_value, input_file, share_y_predict_dict, share_y_predict_proba_dict): print(&quot;子进程执行中&gt;&gt;&gt; pid=&#123;0&#125;,ppid=&#123;1&#125;&quot;.format(os.getpid(),os.getppid())) svc = svm.SVC(probability=True) parameters = &#123;&#x27;kernel&#x27;: [&#x27;rbf&#x27;], &#x27;C&#x27;:map(lambda x:2**x,np.linspace(-2,5,7)), &#x27;gamma&#x27;:map(lambda x:2**x,np.linspace(-5,2,7))&#125; clf = GridSearchCV(svc, parameters, cv=cross_validation_value, n_jobs=CPU_value, scoring=&#x27;accuracy&#x27;) clf.fit(X_train, y_train) C=clf.best_params_[&#x27;C&#x27;] gamma=clf.best_params_[&#x27;gamma&#x27;] print(&#x27;c:&#x27;,C,&#x27;gamma:&#x27;,gamma) y_predict=cross_val_predict(svm.SVC(kernel=&#x27;rbf&#x27;,C=C,gamma=gamma,),X_train,y_train,cv=cross_validation_value,n_jobs=CPU_value) y_predict_prob=cross_val_predict(svm.SVC(kernel=&#x27;rbf&#x27;,C=C,gamma=gamma,probability=True),X_train,y_train,cv=cross_validation_value,n_jobs=CPU_value,method=&#x27;predict_proba&#x27;) input_file = input_file.replace(&quot;.csv&quot;,&quot;&quot;) y_predict_path = input_file + &quot;_predict.csv&quot; y_predict_proba_path = input_file + &quot;_predict_proba.csv&quot; share_y_predict_dict[input_file] = y_predict share_y_predict_proba_dict[input_file] = y_predict_prob[:,1] pd.DataFrame(y_predict).to_csv(y_predict_path, header = None, index = False) pd.DataFrame(y_predict_prob[:,1]).to_csv(y_predict_proba_path, header = None, index = False) print(&quot;子进程终止&gt;&gt;&gt; pid=&#123;0&#125;&quot;.format(os.getpid())) if __name__==&quot;__main__&quot;: print(&quot;主进程执行中&gt;&gt;&gt; pid=&#123;0&#125;&quot;.format(os.getpid())) manager = Manager() share_y_predict_dict = manager.dict() share_y_predict_proba_dict = manager.dict() ps=[] if default_l == 1: data = &quot;&quot; x_len = 1000 y_len = 1000 file_len = len(input_files) threshold = file_len/2 for index, input_file in enumerate(input_files): data = pd.read_csv(input_file,header=None) (x_len,y_len) = data.shape X_train = data.iloc[:,0:y_len-1] y_train = data.iloc[:,[y_len-1]] X_train = X_train.values y_train = y_train.values y_train = y_train.reshape(-1) p=Process(target=worker,name=&quot;worker&quot;+str(index),args=(X_train, y_train, cross_validation_value, CPU_value,input_file,share_y_predict_dict,share_y_predict_proba_dict)) ps.append(p) # 开启进程 for index, input_file in enumerate(input_files): ps[index].start() # 阻塞进程 for index, input_file in enumerate(input_files): ps[index].join() ensembling_prediction = 0 ensembling_prediction_proba = 0 for key, value in share_y_predict_dict.items(): ensembling_prediction = ensembling_prediction + value ensembling_prediction = [1 if e &gt; threshold else 0 for e in ensembling_prediction] print(ensembling_prediction) for key, value in share_y_predict_proba_dict.items(): ensembling_prediction_proba = ensembling_prediction_proba + value ensembling_prediction_proba = ensembling_prediction_proba/3.0 print(ensembling_prediction_proba/3.0) ACC=metrics.accuracy_score(y_train,ensembling_prediction) print(&quot;ACC&quot;,ACC) precision, recall, SN, SP, GM, TP, TN, FP, FN = performance(y_train, ensembling_prediction) F1_Score=metrics.f1_score(y_train, ensembling_prediction) F_measure=F1_Score MCC=metrics.matthews_corrcoef(y_train, ensembling_prediction) auc = metrics.roc_auc_score(y_train, ensembling_prediction_proba) pos=TP+FN neg=FP+TN savedata=[str(input_files),&quot;正：&quot;+str(len(y_train[y_train == 1]))+&#x27;负：&#x27;+str(len(y_train[y_train == 1])),&#x27;svm&#x27;,ACC,precision, recall,SN,SP, GM,F_measure,F1_Score,MCC,auc,TP,FN,FP,TN] final_out_to_excel.append(savedata) print(&quot;final_out_to_excel&quot;,final_out_to_excel) pd.DataFrame(ensembling_prediction).to_csv(&quot;voting_prediction_label.csv&quot;, header = None, index = False) pd.DataFrame(ensembling_prediction_proba).to_csv(&quot;voting_prediction_proba_label.csv&quot;, header = None, index = False) pd.DataFrame(final_out_to_excel).to_excel(&#x27;output&#x27;+&#x27;.xlsx&#x27;,sheet_name=&quot;results&quot;,index=False,header=False) print(&quot;主进程终止&quot;) else: data = &quot;&quot; x_len = 1000 y_len = 1000 file_len = len(input_files) threshold = file_len/2 for index, input_file in enumerate(input_files): data = pd.read_csv(input_file,header=None) (x_len,y_len) = data.shape X_train = data.values half_sequence_number = x_len / 2 y_train = np.array([1 if e &lt; half_sequence_number else 0 for (e,value) in enumerate(X_train)]) y_train = y_train.reshape(-1) print(&quot;default y_train: &quot;, y_train) p=Process(target=worker,name=&quot;worker&quot;+str(index),args=(X_train, y_train, cross_validation_value, CPU_value,input_file,share_y_predict_dict,share_y_predict_proba_dict)) ps.append(p) # 开启进程 for index, input_file in enumerate(input_files): ps[index].start() # 阻塞进程 for index, input_file in enumerate(input_files): ps[index].join() ensembling_prediction = 0 ensembling_prediction_proba = 0 for key, value in share_y_predict_dict.items(): ensembling_prediction = ensembling_prediction + value ensembling_prediction = [1 if e &gt; threshold else 0 for e in ensembling_prediction] print(ensembling_prediction) for key, value in share_y_predict_proba_dict.items(): ensembling_prediction_proba = ensembling_prediction_proba + value ensembling_prediction_proba = ensembling_prediction_proba/3.0 print(ensembling_prediction_proba/3.0) ACC=metrics.accuracy_score(y_train,ensembling_prediction) print(&quot;ACC&quot;,ACC) precision, recall, SN, SP, GM, TP, TN, FP, FN = performance(y_train, ensembling_prediction) F1_Score=metrics.f1_score(y_train, ensembling_prediction) F_measure=F1_Score MCC=metrics.matthews_corrcoef(y_train, ensembling_prediction) auc = metrics.roc_auc_score(y_train, ensembling_prediction_proba) pos=TP+FN neg=FP+TN savedata=[str(input_files),&quot;正：&quot;+str(len(y_train[y_train == 1]))+&#x27;负：&#x27;+str(len(y_train[y_train == 1])),&#x27;svm&#x27;,ACC,precision, recall,SN,SP, GM,F_measure,F1_Score,MCC,auc,TP,FN,FP,TN] final_out_to_excel.append(savedata) print(&quot;final_out_to_excel&quot;,final_out_to_excel) pd.DataFrame(ensembling_prediction).to_csv(&quot;voting_prediction_label.csv&quot;, header = None, index = False) pd.DataFrame(ensembling_prediction_proba).to_csv(&quot;voting_prediction_proba_label.csv&quot;, header = None, index = False) pd.DataFrame(final_out_to_excel).to_excel(&#x27;output&#x27;+&#x27;.xlsx&#x27;,sheet_name=&quot;results&quot;,index=False,header=False) print(&quot;主进程终止&quot;)]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pix2pix tensorflow personal]]></title>
    <url>%2F2018%2F12%2F19%2F2018-12-20-pix2pix_tensorflow_personal%2F</url>
    <content type="text"><![CDATA[本文采用的是tensorflow的eager执行方式对风格迁移算法实现单张图片作为target的训练和测试的过程，由于采用此种的方式的是动态图的实现，因此可以在Jupyter notebook上运行，以下内容是jupyter上的运行结果。 123456789import tensorflow as tftf.enable_eager_execution()import osimport timeimport numpy as npimport matplotlib.pyplot as pltimport PILfrom IPython.display import clear_output 1234path_to_zip = tf.keras.utils.get_file(&quot;facades.tar.gz&quot;, cache_subdir = os.path.abspath(&#x27;.&#x27;), origin=&quot;https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/facades.tar.gz&quot;, extract=True)PATH = os.path.join(os.path.dirname(path_to_zip),&#x27;facades/&#x27;) 12345BUFFER_SIZE = 400BATCH_SIZE = 1IMG_WIDTH = 256IMG_HEIGHT = 256 12345678910111213141516171819202122232425262728293031323334353637383940def load_image(image_file, is_train): image = tf.read_file(image_file) image = tf.image.decode_jpeg(image) w = tf.shape(image)[1] w = w//2 real_image = image[:, :w, :] input_image = image[:, w:, :] input_image = tf.cast(input_image, tf.float32) real_image = tf.cast(real_image, tf.float32) if is_train: input_image = tf.image.resize_images(input_image, [286,286], align_corners=True, method = tf.image.ResizeMethod.NEAREST_NEIGHBOR) real_image = tf.image.resize_images(real_image, [286, 286], align_corners=True, method = tf.image.ResizeMethod.NEAREST_NEIGHBOR) stacked_image = tf.stack([input_image, real_image], axis = 0) cropped_image = tf.random_crop(stacked_image, size = [2, IMG_HEIGHT, IMG_WIDTH, 3]) input_image, real_image = cropped_image[0], cropped_image[1] if np.random.random() &gt; 0.5: input_image = tf.image.flip_left_right(input_image) real_image = tf.image.flip_left_right(real_image) else: input_image = tf.image.resize_images(input_image, size = [IMG_HEIGHT, IMG_WIDTH], align_corners = True, method = 2) real_image = tf.image.resize_images(real_image, size = [IMG_HEIGHT, IMG_WIDTH], align_corners = True, method = 2) input_image = (input_image / 127.5) - 1 real_image = (real_image / 127.5) -1 return input_image, real_image 1234train_dataset = tf.data.Dataset.list_files(PATH + &#x27;train/*.jpg&#x27;)train_dataset = train_dataset.shuffle(BUFFER_SIZE)train_dataset = train_dataset.map(lambda x:load_image(x, True))train_dataset = train_dataset.batch(1) 123test_dataset = tf.data.Dataset.list_files(PATH + &#x27;test/*.jpg&#x27;)test_dataset = test_dataset.map(lambda x:load_image(x, False))test_dataset = test_dataset.batch(1) 1OUTPUT_CHANNELS = 3 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899class Downsample(tf.keras.Model): def __init__(self, filters, size, apply_batchnorm = True): super(Downsample, self).__init__() self.apply_batchnorm = apply_batchnorm initializer = tf.random_normal_initializer(0., 0.02) self.conv1 = tf.keras.layers.Conv2D(filters, (size,size), strides=2, padding=&#x27;same&#x27;, kernel_initializer= initializer, use_bias=False ) if self.apply_batchnorm: self.batchnorm = tf.keras.layers.BatchNormalization() def call(self, x, training): x = self.conv1(x) if self.apply_batchnorm: x = self.batchnorm(x, training = training) x = tf.nn.leaky_relu(x) return xclass Upsample(tf.keras.Model): def __init__(self, filters, size, apply_dropout = False): super(Upsample, self).__init__() self.apply_dropout = apply_dropout initializer = tf.random_normal_initializer(0., 0.02) self.up_conv = tf.keras.layers.Conv2DTranspose(filters, (size,size), strides = 2, padding = &#x27;same&#x27;, kernel_initializer = initializer, use_bias = False) self.batchnorm = tf.keras.layers.BatchNormalization() if self.apply_dropout: self.dropout = tf.keras.layers.Dropout(0.5) def call(self, x1, x2, training): x = self.up_conv(x1) x = self.batchnorm(x, training = training) if self.apply_dropout: x = self.dropout(x, training = training) x = tf.nn.relu(x) x = tf.concat([x, x2], axis = -1) return xclass Generator(tf.keras.Model): def __init__(self): super(Generator, self).__init__() initializer = tf.random_normal_initializer(0., 0.02) self.down1 = Downsample(64, 4, apply_batchnorm = False) self.down2 = Downsample(128, 4) self.down3 = Downsample(256, 4) self.down4 = Downsample(512, 4) self.down5 = Downsample(512, 4) self.down6 = Downsample(512, 4) self.down7 = Downsample(512, 4) self.down8 = Downsample(512, 4) self.up1 = Upsample(512, 4, apply_dropout = True) self.up2 = Upsample(512, 4, apply_dropout = True) self.up3 = Upsample(512, 4, apply_dropout = True) self.up4 = Upsample(512, 4) self.up5 = Upsample(256, 4) self.up6 = Upsample(128, 4) self.up7 = Upsample(64, 4) self.last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, (4, 4), strides = 2, padding = &#x27;same&#x27;, kernel_initializer = initializer) @tf.contrib.eager.defun def call(self, x, training): x1 = self.down1(x, training = training) x2 = self.down2(x1, training = training) x3 = self.down3(x2,training = training) x4 = self.down4(x3, training = training) x5 = self.down5(x4, training = training) x6 = self.down6(x5, training = training) x7 = self.down7(x6, training = training) x8 = self.down8(x7, training = training) x9 = self.up1(x8, x7, training = training) x10 = self.up2(x9, x6, training = training) x11 = self.up3(x10, x5, training = training) x12 = self.up4(x11, x4, training = training) x13 = self.up5(x12, x3, training = training) x14 = self.up6(x13, x2, training = training) x15 = self.up7(x14, x1, training = training) x16 = self.last(x15) x16 = tf.nn.tanh(x16) return x16 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162class DiscDownsample(tf.keras.Model): def __init__(self, filters, size, apply_batchnorm = True): super(DiscDownsample, self).__init__() self.apply_batchnorm = apply_batchnorm initializer = tf.random_normal_initializer(0., 0.02) self.conv1 = tf.keras.layers.Conv2D(filters, (size, size), strides = 2, padding = &#x27;same&#x27;, kernel_initializer = initializer, use_bias = False) if self.apply_batchnorm: self.batchnorm = tf.keras.layers.BatchNormalization() def call(self, x, training): x = self.conv1(x) if self.apply_batchnorm: x = self.batchnorm(x, training = training) x= tf.nn.leaky_relu(x) return xclass Discriminator(tf.keras.Model): def __init__(self): super(Discriminator, self).__init__() initializer = tf.random_normal_initializer(0., 0.02) self.down1 = DiscDownsample(64, 4, False) self.down2 = DiscDownsample(128, 4) self.down3 = DiscDownsample(256, 4) self.zero_pad1 = tf.keras.layers.ZeroPadding2D() self.conv = tf.keras.layers.Conv2D(512, (4, 4), strides = 1, kernel_initializer = initializer, use_bias = False) self.batchnorm1 = tf.keras.layers.BatchNormalization() self.zero_pad2 = tf.keras.layers.ZeroPadding2D() self.last = tf.keras.layers.Conv2D(1, (4, 4), strides = 1, kernel_initializer = initializer) @tf.contrib.eager.defun def call(self, inp, tar, training): x = tf.concat([inp, tar], axis = 1) x = self.down1(x, training = training) x = self.down2(x, training = training) x = self.down3(x, training = training) x = self.zero_pad1(x) x = self.conv(x) x = self.batchnorm1(x, training = training) x = tf.nn.leaky_relu(x) x = self.zero_pad2(x) x = self.last(x) return x 12generator = Generator()discriminator = Discriminator() 1LAMBDA = 100 12345678910def discriminator_loss(disc_real_output, disc_generated_output): real_loss = tf.losses.sigmoid_cross_entropy(multi_class_labels = tf.ones_like(disc_real_output), logits = disc_real_output) generated_loss = tf.losses.sigmoid_cross_entropy(multi_class_labels= tf.zeros_like(disc_generated_output), logits = disc_generated_output) total_disc_loss = real_loss + generated_loss return total_disc_loss 1234567def generator_loss(disc_generated_output, gen_output, target): gan_loss = tf.losses.sigmoid_cross_entropy(multi_class_labels=tf.ones_like(disc_generated_output), logits = disc_generated_output) l1_loss = tf.reduce_mean(tf.abs(target - gen_output)) total_gen_loss = gan_loss + (LAMBDA * l1_loss) return total_gen_loss 12generator_optimizer = tf.train.AdamOptimizer(2e-4, beta1 = 0.5)discriminator_optimizer = tf.train.AdamOptimizer(2e-4, beta1 = 0.5) 123456checkpoint_dir = &#x27;./training_checkpoints&#x27;checkpoint_prefix = os.path.join(checkpoint_dir, &#x27;ckpt&#x27;)checkpoint = tf.train.Checkpoint(generator_optimizer = generator_optimizer, discriminator_optimizer = discriminator_optimizer, generator = generator, discriminator = discriminator) 1EPOCHS = 200 123456789101112131415def generated_images(model, test_input, tar): predictions = model(test_input, training = True) plt.figure(figsize=(15,15)) display_list = [test_input[0], tar[0], predictions[0]] title = [&#x27;Input Image&#x27;, &#x27;Ground Truth&#x27;, &#x27;Predicted Image&#x27;] for i in range(3): plt.subplot(1,3, i+1) plt.title(title[i]) plt.imshow(display_list[i] *0.5 +0.5) plt.axis(&#x27;off&#x27;) plt.show() 123456789101112131415161718192021222324252627282930313233def train(dataset, epochs): for epoch in range(epochs): start = time.time() for input_image, target in dataset: with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape: gen_output = generator(input_image, training = True) disc_real_output = discriminator(input_image, target, training = True) disc_generated_output = discriminator(input_image, gen_output, training = True) gen_loss = generator_loss(disc_generated_output, gen_output, target) disc_loss = discriminator_loss(disc_real_output, disc_generated_output) generator_gradients = gen_tape.gradient(gen_loss, generator.variables) discriminator_gradients = disc_tape.gradient(disc_loss, discriminator.variables) generator_optimizer.apply_gradients(zip(generator_gradients, generator.variables)) discriminator_optimizer.apply_gradients(zip(discriminator_gradients, discriminator.variables)) if epoch %1 == 0: clear_output(wait=True) for inp, tar in test_dataset.take(1): generated_images(generator, inp, tar) if (epoch + 1) % 20 == 0: checkpoint.save(file_prefix = checkpoint_prefix) print(&quot;Time taken for epoch &#123;&#125; is &#123;&#125; sec\n&quot;.format(epoch + 1, time.time() -start)) 1train(train_dataset, EPOCHS) Time taken for epoch 200 is 56.112149238586426 sec 1checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir)) &lt;tensorflow.python.training.checkpointable.util.CheckpointLoadStatus at 0x7f5b8c628d68&gt; 12for inp, tar in test_dataset: generated_images(generator, inp, tar)]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ZigZag Conversion]]></title>
    <url>%2F2018%2F12%2F04%2F2018-12-4-ZigZag_Conversion%2F</url>
    <content type="text"><![CDATA[ZigZag Conversionorignal problem description problem1234567891011The string &quot;PAYPALISHIRING&quot; is written in a zigzag pattern on a given number of rows like this: (you may want to display this pattern in a fixed font for better legibility)P A H NA P L S I I GY I RAnd then read line by line: &quot;PAHNAPLSIIGYIR&quot;Write the code that will take a string and make this conversion given a number of rows:string convert(string s, int numRows); Example 112Input: s &#x3D; &quot;PAYPALISHIRING&quot;, numRows &#x3D; 3Output: &quot;PAHNAPLSIIGYIR&quot; Example 212345678Input: s &#x3D; &quot;PAYPALISHIRING&quot;, numRows &#x3D; 4Output: &quot;PINALSIGYAHRPI&quot;Explanation:P I NA L S I GY A H RP I answer12345678910111213141516171819202122232425262728293031class Solution: def convert(self, s, numRows): &quot;&quot;&quot; :type s: str :type numRows: int :rtype: str &quot;&quot;&quot; row_values = [&quot;&quot;]*numRows s_length = len(s) if numRows == 1: print(&quot;&quot;.join(row_values)) return s for index, value in enumerate(row_values): length = index mark = 1 while length &lt; s_length: if index != 0 and index != (numRows-1): row_values[index] += s[length] if mark == 1 : length += (numRows - index-1)*2 elif mark == 2: length += (index)*2 mark = 2/mark else: row_values[index] += s[length] length += (numRows-1)*2 print(&quot;&quot;.join(row_values)) return &quot;&quot;.join(row_values)]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[downloading images from google]]></title>
    <url>%2F2018%2F11%2F13%2F2018-11-13-downloading-images-from-google%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116&quot;&quot;&quot;really used in fetching url from google images&quot;&quot;&quot;import refrom selenium import webdriverimport timeimport osimport sysimport astimport refrom bs4 import BeautifulSoupimport randomfrom selenium.webdriver.chrome.options import Optionsimport pandas as pdimport numpy as npif __name__ == &quot;__main__&quot;:a pd_data = pd.read_excel(&quot;cleaning_data.xlsx&quot;, header=None) row = 7 whole_data = [] for i_index in range(row): if i_index != 1: for line in pd_data[20:31][i_index].values: if str(line).strip() != None and str(line).strip() != &quot;&quot; and str(line).strip() != &quot;nan&quot;: print(line.strip()) whole_data.append(line.strip()) print(len(whole_data)) length_whole_data = len(whole_data) for index_whole_data in range(length_whole_data): wikiart_path = whole_data[index_whole_data] #&quot;wikiart&quot; original_url = &#x27;https://www.google.co.jp/search?q=&#x27; + wikiart_path + &#x27;&amp;safe=active&amp;rlz=1C1GCEU_zh-CNJP821&amp;source=lnms&amp;tbm=isch&amp;sa=X&amp;ved=0ahUKEwjYhuif2dDeAhWLF3IKHQIvD1gQ_AUIFCgC&amp;biw=1920&amp;bih=1088&#x27; temp_path = wikiart_path + &quot;/&quot; + &quot;temp_google_img_asserts_all2.txt&quot; path = wikiart_path + &quot;/&quot; + &quot;google_img_asserts_all2.txt&quot; # os.environ[&quot;PATH&quot;] += os.pathsep + &#x27;D:\google-art-downloader-master&#x27; if not os.path.exists(wikiart_path): os.makedirs(wikiart_path) # option = webdriver.ChromeOptions() # option.add_argument(&#x27;--headless&#x27;) # option.add_argument(&#x27;--disable-gpu&#x27;) # browser = webdriver.Chrome(chrome_options = option) fireFoxOptions = webdriver.FirefoxOptions() fireFoxOptions.set_headless() browser = webdriver.Firefox(firefox_options=fireFoxOptions) asserts_all=set() mark_time = 0 last_value = 0 # ------------------test start------------------------ # browser.get(original_url) now_len = 0 pre_len = 0 count_all = 0 try: browser.get(original_url) # js=&quot;var q=document.documentElement.scrollTop=100000&quot; # browser.execute_script(js) while(True): time.sleep(random.randint(1,3)) browser.execute_script(&quot;window.scrollBy(0,1500)&quot;) # print(browser.find_element_by_xpath(&#x27;//*[@id=&quot;smb&quot;]&#x27;)) pageSource = browser.page_source soup = BeautifulSoup(pageSource,&#x27;lxml&#x27;) asserts = soup.find_all(&#x27;div&#x27;, &#123;&quot;class&quot;:&quot;rg_meta&quot;&#125;) for assert_value in asserts: data = re.sub(r&#x27;&lt;.*?&gt;&#x27; ,&quot;&quot;, str(assert_value)) data = ast.literal_eval(data) # print(data.get(&quot;ou&quot;)) with open(temp_path,&#x27;a&#x27;,encoding=&quot;utf-8&quot;) as w_file: w_file.write(str(data.get(&quot;ou&quot;)) + &quot;\n&quot;) asserts_all.add(str(data.get(&quot;ou&quot;))) print(len(asserts_all)) now_len = len(asserts_all) if now_len == pre_len: count_all += 1 else: count_all = 0 if count_all &gt;=10: break if count_all == 8: if browser.find_element_by_id(&quot;smb&quot;) != None and browser.find_element_by_id(&quot;smb&quot;) != &quot;&quot;: browser.find_element_by_id(&quot;smb&quot;).click() pre_len = now_len except Exception as e: print(&quot;global&quot;,e) finally: with open(path,&#x27;w&#x27;,encoding=&quot;utf8&quot;) as write_file: for line in asserts_all: write_file.write(str(line)+&quot;\n&quot;) # pass browser.close()]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[taking image urls from pinterest]]></title>
    <url>%2F2018%2F11%2F13%2F2018-11-13-taking-image-urls-from-pinterest%2F</url>
    <content type="text"><![CDATA[采用的是瀑布流的形式展现图片内容，无需用户翻页，新的图片不断自动加载在页面底端，让用户不断的发现新的图片。Pinterest堪称图片版的Twitter，网民可以将感兴趣的图片在Pinterest保存，其他网友可以关注，也可以转发图片。索尼等许多公司也在Pinterest建立了主页，用图片营销旗下的产品和服务。本文实现了对Pinterest图片的爬取工作。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586&quot;&quot;&quot;really used in fetching url from https://artsandculture.google.com/entity/m0bwbv?categoryid=art-movement&quot;&quot;&quot;import refrom selenium import webdriverimport timeimport osimport sysimport refrom bs4 import BeautifulSoupimport randomfrom selenium.webdriver.chrome.options import Optionstemp_path =&quot;temp_chinese_pinterest_img_asserts_all2.txt&quot;path =&quot;chinese_pinterest_img_asserts_all2.txt&quot;wikiart_path = &#x27;chinese-painting&#x27; #&quot;wikiart&quot;original_url = &#x27;https://www.pinterest.jp/jimmyyeji/%E4%B8%AD%E5%9B%BD%E4%B9%A6%E7%94%BB-chinese-painting/&#x27; # &#x27;https://www.wikiart.org/en/paintings-by-style/cubism?select=featured#!#filterName:featured,viewType:masonry&#x27;# os.environ[&quot;PATH&quot;] += os.pathsep + &#x27;D:\google-art-downloader-master&#x27;if not os.path.exists(wikiart_path): os.makedirs(wikiart_path)# option = webdriver.ChromeOptions()# option.add_argument(&#x27;--headless&#x27;)# option.add_argument(&#x27;--disable-gpu&#x27;)# browser = webdriver.Chrome(chrome_options = option)fireFoxOptions = webdriver.FirefoxOptions()fireFoxOptions.set_headless()browser = webdriver.Firefox(firefox_options=fireFoxOptions)asserts_all=set()mark_time = 0last_value = 0# ------------------test start------------------------# browser.get(original_url)now_len = 0pre_len = 0count__all = 0try: browser.get(original_url)# js=&quot;var q=document.documentElement.scrollTop=100000&quot;# browser.execute_script(js) while(True): time.sleep(random.randint(1,3)) browser.execute_script(&quot;window.scrollBy(0,300)&quot;) pageSource = browser.page_source soup = BeautifulSoup(pageSource,&#x27;lxml&#x27;) asserts = soup.find_all(&#x27;img&#x27;) for assert_value in asserts: if assert_value.get(&quot;src&quot;) != None and assert_value.get(&quot;src&quot;) != &quot;&quot; and assert_value.get(&quot;src&quot;).find(&quot;236x&quot;) != -1: print(re.sub(r&#x27;236x&#x27;,&quot;originals&quot;,assert_value.get(&quot;src&quot;))) with open(temp_path,&#x27;a&#x27;,encoding=&quot;utf-8&quot;) as w_file: w_file.write(str(re.sub(r&#x27;236x&#x27;,&quot;originals&quot;,assert_value.get(&quot;src&quot;))) + &quot;\n&quot;) asserts_all.add(re.sub(r&#x27;236x&#x27;,&quot;originals&quot;,assert_value.get(&quot;src&quot;))) print(len(asserts_all)) now_len = len(asserts_all) if now_len == pre_len: count_all += 1 else: count_all = 0 if count_all &gt;=10: break pre_len = now_len with open(path,&#x27;w&#x27;,encoding=&quot;utf8&quot;) as write_file: for line in asserts_all: write_file.write(str(line)+&quot;\n&quot;)except Exception as e: print(&quot;global&quot;,e)finally: browser.close()]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[virtual xshell]]></title>
    <url>%2F2018%2F10%2F31%2F2018-10-31-virtual-xshell%2F</url>
    <content type="text"><![CDATA[在爬虫需要桌面的情况下，如果我们使用的是终端的Linux服务器，此时就会陷入两难的境地。因为可以采用模拟桌面的情况来进行一个虚拟桌面的设置，该种情况可以考虑成一个没有硬件资源的虚拟桌面，人是看不到的。但是符合程序运行的需求就可以。 linux上只有字符界面的时候需要模拟xshell1.安装1sudo apt-get install xvfb 2.运行命令1Xvfb -ac :7 -screen 0 1280x1024x8 3.设置环境变量1export DISPLAY&#x3D;:7 4.运行程序123&#x2F;usr&#x2F;bin&#x2F;google-chrome-stable http:&#x2F;&#x2F;www.baidu.com或者firefox http:&#x2F;&#x2F;www.baidu.com]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[downloading images urls from wikiart]]></title>
    <url>%2F2018%2F10%2F26%2F2018-10-26-downloading-images-from-wikiart%2F</url>
    <content type="text"><![CDATA[本文实现的是爬取wikiart上的数据，通过Selenium的方式来实现的动态加载，动态获取图片的urls，同时使用到了BeautifulSoup这个框架来对数据进行处理。Selenium同时需要安装Chrome.exe插件，如果在windows上使用的话。各个平台的情况不一。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&quot;&quot;&quot;really used in fetching url from wikiart&quot;&quot;&quot;from selenium import webdriverimport timeimport osimport refrom bs4 import BeautifulSoupfrom selenium.webdriver.chrome.options import Options# os.environ[&quot;PATH&quot;] += os.pathsep + &#x27;D:\google-art-downloader-master&#x27;chrome_options = Options()# chrome_options.add_argument(&#x27;--headless&#x27;)browser = webdriver.Chrome(chrome_options = chrome_options)asserts_all=set()mark_time = 0last_value = 0try: browser.get(&#x27;https://www.wikiart.org/en/paintings-by-style/ink-and-wash-painting?select=featured#!#filterName:featured,viewType:masonry&#x27;) while mark_time &lt;= 5: pageSource = browser.page_source soup = BeautifulSoup(pageSource,&#x27;lxml&#x27;) asserts = soup.find_all(&#x27;img&#x27;) for assert_value in asserts: if assert_value.get(&quot;src&quot;) != None and assert_value.get(&quot;src&quot;) != &quot;&quot;: asserts_all.add(str(assert_value.get(&quot;src&quot;)).replace(&quot;!Large.jpg&quot;,&quot;&quot;).replace(&quot;!PinterestSmall.jpg&quot;,&quot;&quot;))# print(str(assert_value.get(&quot;src&quot;)).replace(&quot;!Large.jpg&quot;,&quot;&quot;).replace(&quot;!PinterestSmall.jpg&quot;,&quot;&quot;)) # for assert_value in asserts: now_value = len(asserts_all) print(now_value) if last_value == now_value: mark_time += 1 else: mark_time == 0 try: browser.find_element_by_xpath(&#x27;/html/body/div[2]/div[1]/section/main/div[4]/div/div/div[3]/a/span[3]&#x27;).click() except Exception as e: print(e) last_value = now_value time.sleep(4) google_arts_images_urls = set() with open(&quot;wikiart_ink_and_wash_images_urls.txt&quot;,&#x27;w&#x27;,encoding=&quot;utf8&quot;) as write_file: for line in asserts_all: write_file.write(line+&quot;\n&quot;)except Exception as e: print(&quot;global&quot;,e)finally: browser.close()]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scrapy downloading images]]></title>
    <url>%2F2018%2F10%2F26%2F2018-10-26-scrapy-downloading-images%2F</url>
    <content type="text"><![CDATA[之前介绍了自己编写的多进程多线程实现，发现Scrapy这个框架更好，它拥有多线程的速度，并且会提示你有哪些图片没有爬取下来，完成了多少张图片的爬取之类的信息，而且使用起来非常方便，可以直接pip安装。 1234567891011121314151617181920212223242526272829303132333435363738394041#!/usr/bin/env python# -*- coding: utf-8 -*-import scrapy# import codecsimport osfrom bingproxy import BingProxyclass ImagesSpider(scrapy.Spider): name = &quot;images&quot; dir_path = &quot;huaban_bingproxy_big_images&quot; if not os.path.exists(dir_path): os.makedirs(dir_path)# allowed_domains = [&quot;tyst.migu.cn&quot;] start_urls = [] bingProxy = BingProxy() def start_requests(self): with open(&#x27;processing_threading_huaban_big_images_all_urls_part3.txt&#x27;) as url_list: for url in url_list: url = url.strip() #yield scrapy.Request(url = self.bingProxy.get_proxy_url(url), meta = &#123;&quot;origin_rul&quot;: url&#125;, callback = self.parse ) if url != &quot;&quot; and url != None: yield scrapy.Request(url = url, callback=self.parse,method=&quot;get&quot;) #def __init__(self, urlfile=None,*args, **kwargs): # super(MusicSpider, self).__init__(*args, **kwargs) # uf = codecs.open(urlfile, &#x27;r&#x27;, &#x27;utf-8&#x27;) # urls = [line.strip() for line in uf.readlines()] #self.start_urls = urls def parse(self, response): path = &quot;huaban_bingproxy_big_images&quot; +&quot;/&quot;+response.url.split(&#x27;/&#x27;)[-1] + &quot;.png&quot; # path = path.split(&#x27;?&#x27;)[0] # self.logger.info(&#x27;Saving mp3 %s&#x27;, path) with open(path, &#x27;wb&#x27;) as f: f.write(response.body)]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Using multiple processing to download images based on image URLS]]></title>
    <url>%2F2018%2F10%2F24%2F2018-10-24-Using-multiple-processing-to-download-images-based-on-image-URLS%2F</url>
    <content type="text"><![CDATA[本文中实现的是在获取到所有大数据量的图片urls信息之后，如果抓取图片到本地的方式。因为单机单线程的方式效率非常低，因此考虑到这种情况便编写了一个多进程多线程的爬虫方式，可以很快速的以超n倍单线程的速度进行图片爬取。（后来发现Scrapy框架也能够以超高的速度进行下载）。由于本人在微软实习需要的测试数据量是非常巨大的，这种情况肯定是会被网站封杀和禁止的，好在内部集成了一个几千台的电脑集群，可以通过一个接口调用进行任务分担的情况。因为是接口的形式，所以也可以在该代码中应用，只需要修改downloading_images函数即可。读者若有类似的情况可以参考。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758#coding=utf-8from time import sleep, ctime import threadingimport urllib.requestimport osimport timeprint(&quot;program start %s&quot;%ctime())path = &quot;huaban_tem_images&quot;if not os.path.exists(path): os.makedirs(path)url_set = []filename_set = []temp_url_set = []temp_filename_set = []with open(&quot;images_urls.txt&quot;,&#x27;r&#x27;,encoding=&quot;utf8&quot;) as read_file: all_lines = read_file.readlines() print(&quot;number: &quot;,len(all_lines)) len_all_lines = len(all_lines) for index, line in enumerate(all_lines): url = line.strip() temp_url_set.append(url) file_name = path + &quot;/&quot; +str(url.split(&quot;/&quot;)[-1]) + &quot;.png&quot; temp_filename_set.append(file_name) if (index + 1) % 10 == 0 or index == (len_all_lines - 1): url_set.append(temp_url_set) filename_set.append(temp_filename_set) temp_url_set = [] temp_filename_set = []def downloading_images(url, filename):# print(&quot;start downloading&quot;, url) urllib.request.urlretrieve(url, filename = filename) sleep(1) print(&quot;end downloading&quot;, url)if __name__ == &#x27;__main__&#x27;: epoch = 1 for (urls, filenames) in zip(url_set, filename_set): threads = [] for (url, filename) in zip(urls, filenames): t = threading.Thread(target=downloading_images, args=(url, filename)) threads.append(t) for i in range(len(threads)): threads[i].start() for i in range(len(threads)): threads[i].join() print(&quot;epoch %d finished in %s&quot;%(epoch, ctime())) epoch += 1 print(&#x27;program end:%s&#x27; %ctime())]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[fetch google arts and culture big images from urls]]></title>
    <url>%2F2018%2F10%2F23%2F2018-10-23-fetch-google-arts-and-culture-big-images-from-urls%2F</url>
    <content type="text"><![CDATA[从goole arts and culture big images 抓取大图片，超高清图片，因为google arts and culture对一张具有很高艺术价值的图片的显示方式做过前端的分割处理，因此很难爬取到原图的url，这是一个将浏览器的页面设置到非常大然后截图的形式，同样具有非常高的清晰度。唯一的特征是截取之后的图片会占用大量的内存。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173# This program is applied to take 4k images from google art and culture websitefrom selenium import webdriverimport os, shutilimport time as tfrom PIL import Image, ImageChopsimport tkinter as tkfrom threading import Threadfrom tkinter import filedialogfrom ctypes import windllexImg_value = 1def is_picture(counter): im = Image.open(&#x27;temp/scrapping/image&#x27; + str(counter) + &#x27;.png&#x27;) rgb_im = im.convert(&#x27;RGB&#x27;) r, g, b = rgb_im.getpixel((2000, 1300)) if r == 255 and g == 255 and b == 255: return False else: return Truedef is_same(counter): if counter &gt; 0: prev_counter = counter - 1 new_file = os.path.getsize(&#x27;temp/scrapping/image%s.png&#x27; % str(counter)) old_file = os.path.getsize(&#x27;temp/scrapping/image%s.png&#x27; % str(prev_counter)) os.remove(&#x27;temp/scrapping/image%s.png&#x27; % str(prev_counter)) if new_file == old_file: return True else: return Falsedef trim(image): bg = Image.new(image.mode, image.size, image.getpixel((0, 0))) diff = ImageChops.difference(image, bg) diff = ImageChops.add(diff, diff, 2.0, -100) bbox = diff.getbbox() if bbox: return image.crop(bbox)def remove(value, delete_chars): for c in delete_chars: value = value.replace(c, &#x27;&#x27;) return valuedef file_save(name, status): path = status f = filedialog.asksaveasfile(mode=&#x27;wb&#x27;, defaultextension=&quot;.png&quot;, title=&quot;Saving picture&quot;, initialfile=name, filetypes=((&quot;PNG high resolution image&quot;, &quot;*.png&quot;), (&quot;all files&quot;, &quot;*.*&quot;))) if f is None: return if os.path.abspath(path) != f.name.replace(&#x27;/&#x27;, &#x27;\\&#x27;): im = Image.open(path) im.save(f) os.remove(path) f.close() else: pass def initialize_folders(): if not os.path.exists(&#x27;temp&#x27;): os.makedirs(&#x27;temp&#x27;) else: shutil.rmtree(&#x27;temp&#x27;) if not os.path.exists(&#x27;temp/scrapping&#x27;): os.makedirs(&#x27;temp/scrapping&#x27;)def do_scrapping(url): old_url = url url = &#x27;&#x27; for char in old_url: if char == &#x27;?&#x27;: break url += char options = webdriver.ChromeOptions() options.add_argument(&#x27;--headless&#x27;) options.add_argument(&#x27;--disable-gpu&#x27;) driver = webdriver.Chrome(executable_path=r&quot;chromedriver.exe&quot;, chrome_options=options) driver.set_window_size(4000, 4000) driver.get(url) xPath3 = r&quot;.//html/body/div[3]/div[3]/div/div/div/div[3]/div&quot; # img xPath xPath2 = r&quot;.//html/body/div[3]/div[3]/div/div/div[2]/div[1]/div[2]/div[1]/div&quot; # zoom xPath xPath1 = r&quot;.//html/body/div[3]/div[3]/div/div/div[3]/div/content/span&quot; # open img xPath image_appeared = False # flag for starting click on image image_zoom_taked = False last_file = &#x27;&#x27; # last succeed file driver.implicitly_wait(1) try: authorPic = driver.find_element_by_xpath(r&#x27;/html[1]/body[1]/div[3]/div[3]/div[1]/div[1]/div[6]/section[2]/div[1]/ul[1]/li[2]/a[1]&#x27;).text # author of the picture xPath except Exception: authorPic = &#x27;&#x27; try: name_pic = driver.find_element_by_xpath(r&#x27;/html[1]/body[1]/div[3]/div[3]/div[1]/div[1]/div[6]/section[2]/div[1]/ul[1]/li[1]&#x27;).text[7::] # name of the picture xPath if authorPic != &#x27;&#x27;: name_pic = &#x27; - &#x27; + name_pic except Exception: name_pic = driver.title[0:-23] name_file = authorPic + name_pic name_file = remove(name_file, &#x27;\/:*?&quot;&lt;&gt;|&#x27;) t.sleep(3) for i in range(0, 45): # 45 attempts t.sleep(1) if image_appeared: t.sleep(3) if exImg_value == 1: elem2 = driver.find_element_by_xpath(xPath1) else: elem2 = driver.find_element_by_xpath(xPath2) elem3 = driver.find_element_by_xpath(xPath3) driver.execute_script(&quot;arguments[0].click();&quot;, elem2) driver.execute_script(&quot;arguments[0].click();&quot;, elem3) t.sleep(3) image_appeared = False image_zoom_taked = True else: pass driver.save_screenshot(&#x27;temp/scrapping/image%s.png&#x27; % str(i)) if is_picture(i) and not image_zoom_taked: image_appeared = True if is_same(i): last_file = &#x27;temp/scrapping/image%s.png&#x27; % str(i) break driver.quit() return last_file, name_filedef do_finally_changes(last_file, name_file): if last_file != &#x27;&#x27;: shutil.copyfile(last_file, &#x27;temp/image_result.png&#x27;) shutil.rmtree(&#x27;temp/scrapping&#x27;) imOp = Image.open(&#x27;temp/image_result.png&#x27;) if exImg_value == 1: im = imOp.crop((0, 50, 4000, 4000)) # 20!8 else: im = imOp im = trim(im) im.save(&quot;Ukiyo/&quot; + name_file + &#x27;.png&#x27;) shutil.rmtree(&#x27;temp&#x27;) return name_file return &#x27;An error occurred with processing image&#x27;def start_process(index, url): print(&quot;initialize_folders()&quot;) initialize_folders() print(&quot;do_scrapping(&#123;0&#125;)&quot;.format(url)) file, name = do_scrapping(url) print(&quot;saving image &#123;0&#125;&quot;.format(str(url.split(&quot;/&quot;)[-1])+ &#x27;.png&#x27;)) status = do_finally_changes(file, str(url.split(&quot;/&quot;)[-1]))# file_save(status + &#x27;.png&#x27;, status + &#x27;.png&#x27;) with open(&quot;log.txt&quot;,&quot;a&quot;,encoding=&quot;utf8&quot;) as log_file: log_file.write(str(index)+&quot; : &quot;+url+&quot;\n&quot;)def start(): with open(&quot;asserts.txt&quot;,&#x27;r&#x27;,encoding=&quot;utf8&quot;) as read_file: for index, line in enumerate(read_file.readlines()): url = &quot;https://artsandculture.google.com&quot; + line.strip() print(index, url) start_process(index, url) path = &quot;Ukiyo&quot;if not os.path.exists(path): os.makedirs(path)start()# start_process(0，&quot;https://artsandculture.google.com/asset/sanjūrokkasen/RQEYzE71xKwOlQ&quot;)]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[fetch huaban big image urls]]></title>
    <url>%2F2018%2F10%2F23%2F2018-10-23-fetch-huaban-big-image-urls%2F</url>
    <content type="text"><![CDATA[本文中实现的是采用selenium框架爬取huaban网站的图片的urls，方便下一步的下载操作。Selenium是一个动态爬取框架，采用模拟浏览器行为，通过模拟人工控制浏览器行为的一个框架，具体需要网站的数据分布呈现一定的规律性才比较方便。Selenium能够处理静态爬虫爬不到的内容，比如js动态加载之后才能显示的图片。 1234567891011121314151617181920212223242526272829303132333435363738394041424344from selenium import webdriverfrom selenium.webdriver.chrome.options import Optionsimport timeimport osfrom bs4 import BeautifulSoup# os.environ[&quot;PATH&quot;] += os.pathsep + &#x27;D:\google-art-downloader-master&#x27;chrome_options = Options()chrome_options.add_argument(&quot;--disable-gpu&quot;)chrome_options.add_argument(&quot;--headless&quot;)images_all = set()browser = webdriver.Chrome(chrome_options = chrome_options)# browser = webdriver.PhantomJS()try: with open(&quot;huaban_pin_asserts_all.txt&quot;,&#x27;r&#x27;,encoding=&quot;utf8&quot;) as read_file: for index, line in enumerate(read_file.readlines()): url = &quot;http://huaban.com&quot; + line.strip() browser.get(url,) browser.set_page_load_timeout(10000) browser.set_script_timeout(10000)#这两种设置都进行才有效 time.sleep(1) print(index, url) try: img1 = browser.find_element_by_xpath(&#x27;//*[@id=&quot;baidu_image_holder&quot;]/a/img&#x27;) if img1 != None: images_all.add(img1.get_attribute(&#x27;src&#x27;)) except Exception as e: pass try: img2 = browser.find_element_by_xpath(&#x27;//*[@id=&quot;baidu_image_holder&quot;]/img&#x27;) if img2 != None: images_all.add(img2.get_attribute(&#x27;src&#x27;)) except Exception as e: pass time.sleep(1) with open(&quot;huaban_images_all.txt&quot;,&#x27;w&#x27;,encoding=&quot;utf8&quot;) as write_file: for line in images_all: write_file.write(str(line) + &quot;\n&quot;)except Exception as e: browser.close()]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python的内存调试]]></title>
    <url>%2F2018%2F10%2F18%2F2018-10-18-memory-exceed-of-python%2F</url>
    <content type="text"><![CDATA[python内存增加，内存泄漏调试gc，objgraph在调试深度学习（deep Learning）的算法运行过程中发现，在测试阶段，随着图片数据的增加，迭代的过程中造成内存不断的增长，最终导致内存爆满，泄露，和程序奔溃的问题，因此通过调试来发现问题，用到了objgraph, gc等插件来发现问题。 12345678910111213### 强制进行垃圾回收 gc.collect() # ### 打印出对象数目最多的 50 个类型信息 # objgraph.show_most_common_types(limit=20) objgraph.show_growth() # objgraph.show_backrefs(objgraph.by_type(&#x27;function&#x27;)[0], max_depth = 10, filename = &#x27;obj.dot&#x27;) # objgraph.show_chain( # objgraph.find_backref_chain( # objgraph.by_type(&#x27;_InputList&#x27;)[0], # objgraph.is_proper_module # ), # filename=&#x27;obj_chain.dot&#x27;) # objgraph.show_backrefs(objgraph.by_type(&#x27;Tensor&#x27;)[0], extra_ignore=(id(gc.garbage),), max_depth = 10, filename = &#x27;del_obj.dot&#x27;) 以此方法生成dot文件之后 123456789import osimport pydotplusimport pydotos.environ[&quot;PATH&quot;] += os.pathsep + &#x27;D:/graphviz/bin&#x27;file_path = &quot;D:/microsoft/ImageCreation/del_obj.dot&quot;output_path = &quot;D:/microsoft/ImageCreation/del_obj.pdf&quot;with open(file_path,&#x27;r&#x27;) as f: graph = pydot.graph_from_dot_data(f.read()) graph[0].write_pdf(output_path) 用pydot将dot转换成pdf可视化观看 另外一种方式，木有尝试过，可能比较简单，直接保存为graph.png格式，省略转换步骤 123import objgraphobjgraph.show_growth() # show the growth of the objectsobjgraph.show_refs(variableName, filename=&#x27;graph.png&#x27;) # show the reference structure of the variable]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python实现svm和使用f-score]]></title>
    <url>%2F2018%2F10%2F12%2F2018-10-12-python-svm%2F</url>
    <content type="text"><![CDATA[使用方法使用python语言实现对于支持向量机（SVM）特征选择的实现，特征选择算法为f-score,该程序的主要有点是可输入文件囊括了csv,libsvm,arff等在序列分类的机器学习领域常用到的格式，其中csv:最后一列为class,libsvm:第一列为class,arff:通常最后一列为类别，其中csv和libsvm中不存在开头，直接是使用的数据。 1python train.py -i 1.csv,2.csv,3.libsvm,4.arff -c 5 其中train.py为程序名称 -i :后面接文件名，可以为csv,libsvm,arff格式，多个文件也可以用，但建议不要，因为特征选择时间通常很长 -c:后面5代表五折交叉验证 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225#!/usr/bin/env python# encoding:utf-8import osimport sysimport getoptimport threadingimport pandas as pdimport mathimport numpy as npfrom time import clockfrom sklearn.feature_selection import f_classiffrom sklearn.externals.joblib import Memoryfrom sklearn import metricsimport easy_excelimport itertoolsfrom sklearn.model_selection import KFold from sklearn import svmfrom sklearn.model_selection import train_test_splitimport mathfrom xgboost import XGBClassifierfrom sklearn.neighbors import KNeighborsClassifierimport easy_excelfrom sklearn.preprocessing import MinMaxScalerfrom sklearn.model_selection import *import sklearn.ensemblefrom sklearn.externals import joblibfrom sklearn.linear_model import LogisticRegressionfrom sklearn import metricsfrom sklearn.metrics import roc_curve, aucimport sysfrom sklearn.model_selection import GridSearchCVfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.naive_bayes import GaussianNB import subprocessfrom sklearn.utils import shuffleimport itertoolsfrom sklearn.ensemble import GradientBoostingClassifierimport sysfrom sklearn.decomposition import PCAfrom sklearn.model_selection import GridSearchCVfrom sklearn.pipeline import Pipelinefrom sklearn.svm import SVC, LinearSVCfrom sklearn.naive_bayes import BernoulliNBfrom sklearn.datasets import load_svmlight_filefrom sklearn.tree import DecisionTreeClassifierfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.linear_model import SGDClassifier, LogisticRegressionfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, \ BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifierdef performance(labelArr, predictArr): #labelArr[i] is actual value,predictArr[i] is predict value TP = 0.; TN = 0.; FP = 0.; FN = 0. for i in range(len(labelArr)): if labelArr[i] == 1 and predictArr[i] == 1: TP += 1. if labelArr[i] == 1 and predictArr[i] == 0: FN += 1. if labelArr[i] == 0 and predictArr[i] == 1: FP += 1. if labelArr[i] == 0 and predictArr[i] == 0: TN += 1. if (TP + FN)==0: SN=0 else: SN = TP/(TP + FN) #Sensitivity = TP/P and P = TP + FN if (FP+TN)==0: SP=0 else: SP = TN/(FP + TN) #Specificity = TN/N and N = TN + FP if (TP+FP)==0: precision=0 else: precision=TP/(TP+FP) if (TP+FN)==0: recall=0 else: recall=TP/(TP+FN) GM=math.sqrt(recall*SP) #MCC = (TP*TN-FP*FN)/math.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)) return precision,recall,SN,SP,GM,TP,TN,FP,FNmem = Memory(&quot;./mycache&quot;)@mem.cachedef get_data(file_name): data = load_svmlight_file(file_name) return data[0], data[1]def csv_and_arff2svm(arff_files): svm_files = [] for arff_file in arff_files: name = arff_file[0: arff_file.rindex(&#x27;.&#x27;)] tpe = arff_file[arff_file.rindex(&#x27;.&#x27;)+1:] svm_file = name+&quot;.libsvm&quot; svm_files.append(svm_file) if tpe == &quot;arff&quot;: if os.path.exists(svm_file): pass else: f = open(arff_file) w = open(svm_file, &#x27;w&#x27;) flag = False for line in f.readlines(): if flag: if line.strip() == &#x27;&#x27;: continue temp = line.strip(&#x27;\n&#x27;).strip(&#x27;\r&#x27;).split(&#x27;,&#x27;) w.write(temp[len(temp)-1]) for i in range(len(temp)-1): w.write(&#x27; &#x27;+str(i+1)+&#x27;:&#x27;+str(temp[i])) w.write(&#x27;\n&#x27;) else: line = line.upper() if line.startswith(&#x27;@DATA&#x27;) or flag: flag = True f.close() w.close() elif tpe == &quot;csv&quot;: if os.path.exists(svm_file): pass else: f = open(arff_file) w = open(svm_file, &#x27;w&#x27;) for line in f.readlines(): if line.strip() == &#x27;&#x27;: continue temp = line.strip(&#x27;\n&#x27;).strip(&#x27;\r&#x27;).split(&#x27;,&#x27;) w.write(temp[len(temp)-1]) for i in range(len(temp)-1): w.write(&#x27; &#x27;+str(i+1)+&#x27;:&#x27;+str(temp[i])) w.write(&#x27;\n&#x27;) f.close() w.close() elif tpe == &quot;libsvm&quot;: continue else: print &quot;File format error! Arff and libsvm are passed.&quot; sys.exit() return svm_filesopts, args = getopt.getopt(sys.argv[1:], &quot;hi:c:t:o:s:m:&quot;, )for op, value in opts: if op == &quot;-i&quot;: input_files = str(value) input_files = input_files.replace(&quot; &quot;, &quot;&quot;).split(&#x27;,&#x27;) for input_file in input_files: if input_file == &quot;&quot;: print &quot;Warning: please insure no blank in your input files !&quot; sys.exit() elif op == &quot;-c&quot;: cv = int(value)if __name__ ==&quot;__main__&quot;: path=&quot;&quot; outputname=&quot;svm_f-score&quot; name=outputname print &#x27;*** Validating file format ...&#x27; input_files = csv_and_arff2svm(input_files) for input_file in input_files: # 导入原始数据 X, Y = get_data(input_file) train_data = X.todense() train_data=np.array(train_data) F, pval = f_classif(train_data, Y) idx = np.argsort(F) selected_list_=idx[::-1] F_sort_value=[F[e] for e in selected_list_] with open(&quot;all_dimension_results.txt&quot;,&#x27;a&#x27;) as f: f.write(str(F_sort_value)+&quot;\n&quot;) print F_sort_value with open(&quot;all_dimension_results.txt&quot;,&#x27;a&#x27;) as f: f.write(str(selected_list_)+&quot;\n&quot;) print selected_list_ bestACC=0 bestC=0 bestgamma=0 best_dimension=0 all_dimension_results=[] for select_num in xrange(1,len(train_data[0])+1): train_data2=train_data print np.array(train_data).shape print np.array(train_data2).shape selected_list_2=selected_list_[xrange(select_num)] X_train=pd.DataFrame(train_data2) X_train=X_train.iloc[:,selected_list_2] X = np.array(X_train) svc = svm.SVC() parameters = &#123;&#x27;kernel&#x27;: [&#x27;rbf&#x27;], &#x27;C&#x27;:map(lambda x:2**x,np.linspace(-2,5,7)), &#x27;gamma&#x27;:map(lambda x:2**x,np.linspace(-5,2,7))&#125; clf = GridSearchCV(svc, parameters, cv=cv, n_jobs=2, scoring=&#x27;accuracy&#x27;) clf.fit(X, Y) C=clf.best_params_[&#x27;C&#x27;] joblib.dump(clf,path+outputname+str(select_num)+&quot;.model&quot;) gamma=clf.best_params_[&#x27;gamma&#x27;] y_predict=cross_val_predict(svm.SVC(kernel=&#x27;rbf&#x27;,C=C,gamma=gamma),X,Y,cv=cv,n_jobs=2) y_predict_prob=cross_val_predict(svm.SVC(kernel=&#x27;rbf&#x27;,C=C,gamma=gamma,probability=True),X,Y,cv=cv,n_jobs=2,method=&#x27;predict_proba&#x27;) predict_save=[Y.astype(int),y_predict.astype(int),y_predict_prob[:,1]] predict_save=np.array(predict_save).T pd.DataFrame(predict_save).to_csv(path+outputname+&quot;_&quot;+&#x27;_predict_crossvalidation.csv&#x27;,header=None,index=False) ROC_AUC_area=metrics.roc_auc_score(Y,y_predict) ACC=metrics.accuracy_score(Y,y_predict) precision, recall, SN, SP, GM, TP, TN, FP, FN = performance(Y, y_predict) F1_Score=metrics.f1_score(Y, y_predict) F_measure=F1_Score MCC=metrics.matthews_corrcoef(Y, y_predict) pos=TP+FN neg=FP+TN savedata=[[[&#x27;svm&#x27;+&quot;C:&quot;+str(C)+&quot;gamma:&quot;+str(gamma),ACC,precision, recall,SN, SP, GM,F_measure,F1_Score,MCC,ROC_AUC_area,TP,FN,FP,TN,pos,neg]]] if ACC&gt;bestACC: bestACC=ACC bestgamma=gamma bestC=C best_dimension=X.shape[1] print savedata print X.shape[1] with open(&quot;all_dimension_results.txt&quot;,&#x27;a&#x27;) as f: f.write(str(savedata)+&quot;\n&quot;) all_dimension_results.append(savedata) print bestACC print bestC print bestgamma print best_dimension easy_excel.save(&quot;svm_crossvalidation&quot;,[str(X.shape[1])],savedata,path+&#x27;cross_validation_&#x27;+name+&#x27;.xls&#x27;)]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分田地-网易-python]]></title>
    <url>%2F2018%2F08%2F27%2F2018-08-27-plite-python%2F</url>
    <content type="text"><![CDATA[题目描述牛牛和 15 个朋友来玩打土豪分田地的游戏，牛牛决定让你来分田地，地主的田地可以看成是一个矩形，每个位置有一个价值。分割田地的方法是横竖各切三刀，分成 16 份，作为领导干部，牛牛总是会选择其中总价值最小的一份田地， 作为牛牛最好的朋友，你希望牛牛取得的田地的价值和尽可能大，你知道这个值最大可以是多少吗？ 输入描述:每个输入包含 1 个测试用例。每个测试用例的第一行包含两个整数 n 和 m（1 &lt;= n, m &lt;= 75），表示田地的大小，接下来的 n 行，每行包含 m 个 0-9 之间的数字，表示每块位置的价值。 输出描述：输出一行表示牛牛所能取得的最大的价值。 示例输入 123454 43332323333322323 输出 12 代码 (不能AC，受限于python，最高20%)12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667(x,y)=[int(e) for e in raw_input(&quot;&quot;).split()]data=[]for i in range(x): data.append([int(e) for e in list(raw_input(&quot;&quot;))])sum_value=[[0 for x_i in range(80)] for y_j in range(80)] for i in range(x+1): for j in range(y+1): if i == 0 or j == 0: sum_value[i][j] = 0 else: sum_value[i][j] = sum_value[i-1][j] + sum_value[i][j-1] -sum_value[i-1][j-1] +data[i-1][j-1] def get_data(x1,y1,x2,y2): return sum_value[x2][y2]-sum_value[x1][y2]-sum_value[x2][y1]+sum_value[x1][y1] def fentiandi(x_length,y_length,avg_value): for index_i in range(1,y_length-2): for index_j in range(index_i+1,y_length-1): for index_k in range(index_j+1,y_length): count_value = 0 pre_line_index=0 for x_index_i in range(1,x_length+1): line1 = get_data(pre_line_index,0,x_index_i,index_i) line2 = get_data(pre_line_index,index_i,x_index_i,index_j) line3 = get_data(pre_line_index,index_j,x_index_i,index_k) line4 = get_data(pre_line_index,index_k,x_index_i,y_length) if line1 &gt;= avg_value and line2 &gt;= avg_value and line3 &gt;= avg_value and line4 &gt;= avg_value: pre_line_index = x_index_i count_value += 1 if count_value &gt;= 4: return True return Falsedef get_min_value(): high=sum_value[x-1][y-1] best_value=0 low=0 while low &lt;= high: middle=(low+high)/2# print &quot;low,middle,high&quot;,low,middle,high if fentiandi(x,y,middle) == True: low=middle+1 best_value=middle else: high = middle-1 print best_valueget_min_value()]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>python</tag>
      </tags>
  </entry>
</search>
